{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d199c213",
   "metadata": {},
   "source": [
    "# Feature Engineering: Creating Derived Variables\n",
    "\n",
    "To improve the predictive power of our model, we generate new features from the raw dataset.  \n",
    "These engineered variables capture useful patterns that may not be obvious in the original columns.\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- **Load the modified dataset** from the local `products_modified.csv` file using `pandas.read_csv`\n",
    "\n",
    "- **Transform product views**:\n",
    "  - `views_log = np.log1p(df[\"number_of_views\"])`\n",
    "  - Applies a log transformation (`log(1 + x)`) to the number of views\n",
    "  - This reduces skewness and makes highly variable counts easier for the model to handle\n",
    "\n",
    "- **Extract temporal features from listing date**:\n",
    "  - `year = pd.to_datetime(df[\"listing_date\"]).dt.year`\n",
    "  - `month = pd.to_datetime(df[\"listing_date\"]).dt.month`\n",
    "  - Converts the listing date into numeric year and month values\n",
    "  - Helps the model capture seasonal or time‚Äërelated trends in product listings\n",
    "\n",
    "By adding these engineered features (`views_log`, `year`, `month`), we enrich the dataset with more informative signals that can improve category prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b77c6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/products_modified.csv\")\n",
    "\n",
    "df[\"views_log\"] = np.log1p(df[\"number_of_views\"])\n",
    "df[\"year\"] = pd.to_datetime(df[\"listing_date\"]).dt.year\n",
    "df[\"month\"] = pd.to_datetime(df[\"listing_date\"]).dt.month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99748b37",
   "metadata": {},
   "source": [
    "# Building the Machine Learning Pipeline\n",
    "\n",
    "Once the dataset is cleaned and features are engineered, we need to define how the data will be transformed and passed into a classifier.  \n",
    "This is done using a **scikit‚Äëlearn Pipeline**, which chains together preprocessing steps and the final model.\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- **Define a text vectorizer**:\n",
    "  - `TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=100000)`\n",
    "  - Converts product titles into numerical features using TF‚ÄëIDF\n",
    "  - Captures both single words (unigrams) and pairs of words (bigrams)\n",
    "  - Ignores very rare terms (`min_df=2`) and limits vocabulary size for efficiency\n",
    "\n",
    "- **Create a ColumnTransformer**:\n",
    "  - `\"text\"` applies the TF‚ÄëIDF vectorizer to the `product_title` column\n",
    "  - `\"num\"` applies `StandardScaler` to numeric features (`views_log`, `merchant_rating`, `year`, `month`)\n",
    "  - This ensures text and numeric features are processed appropriately in parallel\n",
    "\n",
    "- **Build the Pipeline**:\n",
    "  - `\"prep\"` step runs the ColumnTransformer (text + numeric preprocessing)\n",
    "  - `\"clf\"` step trains a `LogisticRegression` classifier\n",
    "    - `max_iter=2000` allows more iterations for convergence\n",
    "    - `n_jobs=4` enables parallel computation\n",
    "    - `multi_class=\"multinomial\"` handles multiple categories\n",
    "    - `class_weight=\"balanced\"` adjusts for imbalanced class distributions\n",
    "\n",
    "This pipeline ensures that raw product titles and numeric features are automatically transformed into a format suitable for classification, and then passed into a robust logistic regression model for category prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc7daedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=100000)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", text_vectorizer, \"product_title\"),\n",
    "        (\"num\", StandardScaler(), [\"views_log\", \"merchant_rating\", \"year\", \"month\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, n_jobs=4, multi_class=\"multinomial\", class_weight=\"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca489a7",
   "metadata": {},
   "source": [
    "# Training and Evaluating Multiple Models\n",
    "\n",
    "To identify the best algorithm for product category prediction, we train and compare several classifiers using the same preprocessing pipeline.\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- **Check for missing values** with `df.isna().sum()` and drop rows where `product_title` is missing.  \n",
    "  This ensures that all training samples have valid text input.\n",
    "\n",
    "- **Define features and labels**:\n",
    "  - `X` includes both text (`product_title`) and numeric features (`views_log`, `merchant_rating`, `year`, `month`)\n",
    "  - `y` is the target column `category_label`\n",
    "\n",
    "- **Split the dataset** into training and validation sets using `train_test_split`:\n",
    "  - 80% of the data is used for training\n",
    "  - 20% is reserved for validation\n",
    "  - `stratify=y` ensures class proportions are preserved\n",
    "  - `random_state=42` guarantees reproducibility\n",
    "\n",
    "- **Set up a dictionary of models to test**:\n",
    "  - Logistic Regression\n",
    "  - Naive Bayes\n",
    "  - Decision Tree\n",
    "  - Random Forest\n",
    "  - Linear SVM\n",
    "\n",
    "- **Build pipelines for each model**:\n",
    "  - For **Naive Bayes**, only text features are used (TF‚ÄëIDF vectorization), since it does not handle scaled numeric features well\n",
    "  - For the other models, both text (TF‚ÄëIDF) and numeric features (scaled with `StandardScaler`) are included via `ColumnTransformer`\n",
    "\n",
    "- **Train and evaluate each model**:\n",
    "  - `pipeline.fit(X_train, y_train)` trains the model\n",
    "  - `pipeline.predict(X_val)` generates predictions on the validation set\n",
    "  - `accuracy_score` reports the overall accuracy\n",
    "  - `classification_report` provides precision, recall, and F1‚Äëscore for each category\n",
    "\n",
    "This loop allows us to quickly compare multiple algorithms under the same preprocessing setup, helping us identify which classifier performs best for product category prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f361d05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0         0\n",
      "product_id         0\n",
      "product_title      0\n",
      "merchant_id        0\n",
      "category_label     0\n",
      "product_code       0\n",
      "number_of_views    0\n",
      "merchant_rating    0\n",
      "listing_date       0\n",
      "views_log          0\n",
      "year               0\n",
      "month              0\n",
      "dtype: int64\n",
      "\n",
      "üîç Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uvirg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9247698504027618\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "             CPU       0.00      0.00      0.00        16\n",
      "            CPUs       0.98      0.94      0.96       742\n",
      " Digital Cameras       1.00      0.99      0.99       532\n",
      "     Dishwashers       0.91      0.96      0.93       675\n",
      "        Freezers       0.87      0.95      0.91       436\n",
      " Fridge Freezers       0.96      0.89      0.93      1085\n",
      "         Fridges       0.85      0.80      0.83       681\n",
      "      Microwaves       0.98      0.97      0.97       461\n",
      "    Mobile Phone       0.03      0.09      0.04        11\n",
      "   Mobile Phones       0.93      0.95      0.94       794\n",
      "             TVs       0.99      0.97      0.98       701\n",
      "Washing Machines       0.98      0.94      0.96       794\n",
      "          fridge       0.04      0.12      0.06        24\n",
      "\n",
      "        accuracy                           0.92      6952\n",
      "       macro avg       0.73      0.74      0.73      6952\n",
      "    weighted avg       0.94      0.92      0.93      6952\n",
      "\n",
      "\n",
      "üîç Naive Bayes\n",
      "Accuracy: 0.9408803222094362\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "             CPU       0.00      0.00      0.00        16\n",
      "            CPUs       0.98      1.00      0.99       742\n",
      " Digital Cameras       1.00      0.99      1.00       532\n",
      "     Dishwashers       0.98      0.95      0.97       675\n",
      "        Freezers       1.00      0.71      0.83       436\n",
      " Fridge Freezers       0.80      0.98      0.88      1085\n",
      "         Fridges       0.91      0.83      0.87       681\n",
      "      Microwaves       0.99      0.97      0.98       461\n",
      "    Mobile Phone       0.00      0.00      0.00        11\n",
      "   Mobile Phones       0.98      0.98      0.98       794\n",
      "             TVs       0.98      0.99      0.99       701\n",
      "Washing Machines       0.98      0.96      0.97       794\n",
      "          fridge       0.00      0.00      0.00        24\n",
      "\n",
      "        accuracy                           0.94      6952\n",
      "       macro avg       0.74      0.72      0.73      6952\n",
      "    weighted avg       0.94      0.94      0.94      6952\n",
      "\n",
      "\n",
      "üîç Decision Tree\n",
      "Accuracy: 0.9201668584579977\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "             CPU       0.00      0.00      0.00        16\n",
      "            CPUs       0.98      0.98      0.98       742\n",
      " Digital Cameras       0.99      0.96      0.98       532\n",
      "     Dishwashers       0.92      0.93      0.92       675\n",
      "        Freezers       0.89      0.91      0.90       436\n",
      " Fridge Freezers       0.90      0.89      0.89      1085\n",
      "         Fridges       0.85      0.81      0.83       681\n",
      "      Microwaves       0.94      0.93      0.94       461\n",
      "    Mobile Phone       0.00      0.00      0.00        11\n",
      "   Mobile Phones       0.93      0.96      0.95       794\n",
      "             TVs       0.96      0.96      0.96       701\n",
      "Washing Machines       0.92      0.93      0.92       794\n",
      "          fridge       0.00      0.00      0.00        24\n",
      "\n",
      "        accuracy                           0.92      6952\n",
      "       macro avg       0.71      0.71      0.71      6952\n",
      "    weighted avg       0.92      0.92      0.92      6952\n",
      "\n",
      "\n",
      "üîç Random Forest\n",
      "Accuracy: 0.9439010356731876\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "             CPU       0.00      0.00      0.00        16\n",
      "            CPUs       0.98      1.00      0.99       742\n",
      " Digital Cameras       1.00      0.98      0.99       532\n",
      "     Dishwashers       0.92      0.96      0.94       675\n",
      "        Freezers       0.93      0.92      0.93       436\n",
      " Fridge Freezers       0.91      0.92      0.92      1085\n",
      "         Fridges       0.89      0.86      0.88       681\n",
      "      Microwaves       0.97      0.97      0.97       461\n",
      "    Mobile Phone       0.00      0.00      0.00        11\n",
      "   Mobile Phones       0.96      0.98      0.97       794\n",
      "             TVs       0.96      0.99      0.98       701\n",
      "Washing Machines       0.95      0.94      0.94       794\n",
      "          fridge       0.00      0.00      0.00        24\n",
      "\n",
      "        accuracy                           0.94      6952\n",
      "       macro avg       0.73      0.73      0.73      6952\n",
      "    weighted avg       0.94      0.94      0.94      6952\n",
      "\n",
      "\n",
      "üîç Linear SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uvirg\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9567031070195627\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "             CPU       0.00      0.00      0.00        16\n",
      "            CPUs       0.98      1.00      0.99       742\n",
      " Digital Cameras       1.00      0.99      1.00       532\n",
      "     Dishwashers       0.93      0.96      0.94       675\n",
      "        Freezers       0.97      0.95      0.96       436\n",
      " Fridge Freezers       0.93      0.95      0.94      1085\n",
      "         Fridges       0.91      0.90      0.91       681\n",
      "      Microwaves       0.98      0.97      0.97       461\n",
      "    Mobile Phone       0.00      0.00      0.00        11\n",
      "   Mobile Phones       0.96      0.99      0.97       794\n",
      "             TVs       0.99      0.98      0.99       701\n",
      "Washing Machines       0.97      0.95      0.96       794\n",
      "          fridge       0.00      0.00      0.00        24\n",
      "\n",
      "        accuracy                           0.96      6952\n",
      "       macro avg       0.74      0.74      0.74      6952\n",
      "    weighted avg       0.95      0.96      0.95      6952\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(df.isna().sum())\n",
    "df = df.dropna(subset=[\"product_title\"])\n",
    "\n",
    "# Features »ôi label\n",
    "X = df[[\"product_title\", \"views_log\", \"merchant_rating\", \"year\", \"month\"]]\n",
    "y = df[\"category_label\"]\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, class_weight=\"balanced\", multi_class=\"multinomial\"),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Linear SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "# Loop through models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n {name}\")\n",
    "    \n",
    "    if name == \"Naive Bayes\":\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocessing\", ColumnTransformer([\n",
    "                (\"text\", TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=100000), \"product_title\")\n",
    "            ])),\n",
    "            (\"classifier\", model)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocessing\", ColumnTransformer([\n",
    "                (\"text\", TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=100000), \"product_title\"),\n",
    "                (\"num\", StandardScaler(), [\"views_log\", \"merchant_rating\", \"year\", \"month\"])\n",
    "            ])),\n",
    "            (\"classifier\", model)\n",
    "        ])\n",
    "    \n",
    "    # Training and assessment\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "    print(classification_report(y_val, y_pred, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
